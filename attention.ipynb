{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'c'] [1 3]\n",
      "['d' 'b'] [4 2]\n",
      "['c' 'a'] [3 1]\n",
      "['d' 'b'] [4 2]\n",
      "['d' 'c'] [4 3]\n",
      "['a' 'b'] [1 2]\n",
      "['b' 'a'] [2 1]\n",
      "['c' 'd'] [3 4]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def zero_pad(X, seq_len):\n",
    "    return np.array([x[:seq_len - 1] + [0] * max(seq_len - len(x), 1) for x in X])\n",
    "\n",
    "\n",
    "def get_vocabulary_size(X):\n",
    "    return max([max(x) for x in X]) + 1  # plus the 0th word\n",
    "\n",
    "\n",
    "def fit_in_vocabulary(X, voc_size):\n",
    "    return [[w for w in x if w < voc_size] for x in X]\n",
    "\n",
    "\n",
    "def batch_generator(X, y, batch_size):\n",
    "    \"\"\"Primitive batch generator \n",
    "    \"\"\"\n",
    "    size = X.shape[0]\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    X_copy = X_copy[indices]\n",
    "    y_copy = y_copy[indices]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i + batch_size <= size:\n",
    "            yield X_copy[i:i + batch_size], y_copy[i:i + batch_size]\n",
    "            i += batch_size\n",
    "        else:\n",
    "            i = 0\n",
    "            indices = np.arange(size)\n",
    "            np.random.shuffle(indices)\n",
    "            X_copy = X_copy[indices]\n",
    "            y_copy = y_copy[indices]\n",
    "            continue\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test batch generator\n",
    "    gen = batch_generator(np.array(['a', 'b', 'c', 'd']), np.array([1, 2, 3, 4]), 2)\n",
    "    for _ in range(8):\n",
    "        xx, yy = next(gen)\n",
    "        print(xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable bidirectional_rnn/fw/gru_cell/gates/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\msy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\msy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\msy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ec6f7056afda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# (Bi-)RNN layer(-s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n\u001b[1;32m---> 47\u001b[1;33m                         inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RNN_outputs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[1;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    401\u001b[0m           \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state_fw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m           time_major=time_major, scope=fw_scope)\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;31m# Backward direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\u001b[0m\n\u001b[0;32m   3207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3209\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3211\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2939\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 2941\u001b[1;33m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2943\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2876\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m   2877\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2878\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2879\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2880\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   3177\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   3178\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 3179\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    782\u001b[0m           \u001b[0mcall_cell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcall_cell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m           \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m           skip_conditionals=True)\n\u001b[0m\u001b[0;32m    785\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_rnn_step\u001b[1;34m(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;31m# steps.  This is faster when max_seq_len is equal to the number of unrolls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;31m# (which is typical for dynamic_rnn).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[0mnew_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;31m# method.  See the class docstring for more details.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[1;32m--> 339\u001b[1;33m                                      *args, **kwargs)\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_shape'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, inputs_shape)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;34m\"gates/%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_depth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         initializer=self._kernel_initializer)\n\u001b[0m\u001b[0;32m    463\u001b[0m     self._gate_bias = self.add_variable(\n\u001b[0;32m    464\u001b[0m         \u001b[1;34m\"gates/%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_BIAS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_variable\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;34m\"\"\"Alias for `add_weight`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m   def add_weight(self, name, shape,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, partitioner)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             getter=vs.get_variable)\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, getter)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         use_resource=use_resource)\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    495\u001b[0m     new_variable = getter(\n\u001b[0;32m    496\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m         **kwargs_for_getter)\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1326\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1328\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1329\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1330\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1088\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 743\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    744\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable bidirectional_rnn/fw/gru_cell/gates/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\msy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\msy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\msy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "NUM_WORDS = 10000\n",
    "INDEX_FROM = 3\n",
    "SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 150\n",
    "ATTENTION_SIZE = 50\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "MODEL_PATH = './model'\n",
    "\n",
    "# Load the data set\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "\n",
    "# Sequences pre-processing\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)\n",
    "\n",
    "# Different placeholders\n",
    "with tf.name_scope('Inputs'):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')\n",
    "\n",
    "# Embedding layer\n",
    "with tf.name_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)\n",
    "\n",
    "# (Bi-)RNN layer(-s)\n",
    "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "tf.summary.histogram('RNN_outputs', rnn_outputs)\n",
    "\n",
    "# Attention layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    tf.summary.histogram('alphas', alphas)\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(attention_output, keep_prob_ph)\n",
    "\n",
    "# Fully connected layer\n",
    "with tf.name_scope('Fully_connected_layer'):\n",
    "    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
    "    b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    tf.summary.histogram('W', W)\n",
    "\n",
    "with tf.name_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Batch generators\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
